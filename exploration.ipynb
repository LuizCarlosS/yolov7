{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k:\\Users\\krish\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from models.yolo import Model\n",
    "from utils.torch_utils import intersect_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "from torch import nn\n",
    "\n",
    "from models.yolo import Model\n",
    "from utils.torch_utils import intersect_dicts\n",
    "\n",
    "yolo_features = {}\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        yolo_features[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "class TrackingHead(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1_3 = nn.Conv2d(in_channels=2048, out_channels=512, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.conv2_3 = nn.Conv2d(in_channels=512, out_channels=64, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.linear1 = nn.Linear(64*4*8, 640*640*1)\n",
    "        yolo_features = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.relu(self.conv1_3(x)))\n",
    "        x = self.bn2(self.relu(self.conv2_3(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.sigmoid(self.linear1(x))\n",
    "        return x\n",
    "\n",
    "class YoloTracker(nn.Module):\n",
    "    def __init__(self,\n",
    "                cutoff_layer=50, \n",
    "                weights = 'yolov7.pt',\n",
    "                device = 'cuda',\n",
    "                cfg = './cfg/training/yolov7.yaml',\n",
    "                hyp = './data/hyp.scratch.p5.yaml',\n",
    "                nc = 80,\n",
    "                window = 2,\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "        with open(hyp) as f:\n",
    "            hyp = yaml.load(f, Loader=yaml.SafeLoader)  # load hyps\n",
    "        \n",
    "        tracking_classes = [0]\n",
    "        pretrained = weights.endswith('.pt')\n",
    "        if pretrained:\n",
    "            ckpt = torch.load(weights, map_location=device)  # load checkpoint\n",
    "            model = Model(cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
    "            exclude = ['anchor'] if (cfg or hyp.get('anchors')) else []  # exclude keys\n",
    "            state_dict = ckpt['model'].float().state_dict()  # to FP32\n",
    "            state_dict = intersect_dicts(state_dict, model.state_dict(), exclude=exclude)  # intersect\n",
    "            model.load_state_dict(state_dict, strict=False)  # load\n",
    "        \n",
    "        self.detection_branch = model\n",
    "        self.yolo_features = {}\n",
    "        self.detection_branch.model[cutoff_layer].register_forward_hook(get_features('features'))\n",
    "        # self.detection_branch = self.detection_branch['model']\n",
    "        self.track = TrackingHead().to(device)\n",
    "        \n",
    "    def forward(self, current_frame):\n",
    "        track = True\n",
    "        if 'features' in self.yolo_features.keys():\n",
    "            last_feats = self.yolo_features['features']\n",
    "            track = False\n",
    "            trk_p = None\n",
    "        yolo_p = self.detection_branch(current_frame)\n",
    "        if track:\n",
    "            trk_p = self.track(torch.cat(last_feats, self.yolo_features['features'], dim=1))\n",
    "        return yolo_p, trk_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_features() missing 1 required positional argument: 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m YoloTracker()\n",
      "Cell \u001b[1;32mIn[1], line 63\u001b[0m, in \u001b[0;36mYoloTracker.__init__\u001b[1;34m(self, cutoff_layer, weights, device, cfg, hyp, nc, window)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetection_branch \u001b[39m=\u001b[39m model\n\u001b[0;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39myolo_features \u001b[39m=\u001b[39m {}\n\u001b[1;32m---> 63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetection_branch\u001b[39m.\u001b[39mmodel[cutoff_layer]\u001b[39m.\u001b[39mregister_forward_hook(get_features(\u001b[39m'\u001b[39;49m\u001b[39mfeatures\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     64\u001b[0m \u001b[39m# self.detection_branch = self.detection_branch['model']\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack \u001b[39m=\u001b[39m TrackingHead()\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mTypeError\u001b[0m: get_features() missing 1 required positional argument: 'name'"
     ]
    }
   ],
   "source": [
    "model = YoloTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = torch.load('./yolov7.pt', map_location='cuda')  # load checkpoint\n",
    "# model = Model(ckpt['model'].yaml, ch=3, nc=80, anchors=3).to('cuda')  # create\n",
    "# exclude = ['anchor']  # exclude keys\n",
    "# state_dict = ckpt['model'].float().state_dict()  # to FP32\n",
    "# state_dict = intersect_dicts(state_dict, model.state_dict(), exclude=exclude)  # intersect\n",
    "# model.load_state_dict(state_dict, strict=False)  # load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from models.yolo import Model\n",
    "from utils.torch_utils import intersect_dicts\n",
    "\n",
    "cutoff_layer=50\n",
    "weights = 'yolov7.pt'\n",
    "device = 'cuda'\n",
    "cfg = './cfg/training/yolov7.yaml'\n",
    "hyp = './data/hyp.scratch.p5.yaml'\n",
    "nc = 80\n",
    "window = 2\n",
    "with open(hyp) as f:\n",
    "    hyp = yaml.load(f, Loader=yaml.SafeLoader)  # load hyps\n",
    "\n",
    "tracking_classes = [0]\n",
    "pretrained = weights.endswith('.pt')\n",
    "if pretrained:\n",
    "    ckpt = torch.load(weights, map_location=device)  # load checkpoint\n",
    "    model = Model(cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
    "    exclude = ['anchor'] if (cfg or hyp.get('anchors')) else []  # exclude keys\n",
    "    state_dict = ckpt['model'].float().state_dict()  # to FP32\n",
    "    state_dict = intersect_dicts(state_dict, model.state_dict(), exclude=exclude)  # intersect\n",
    "    model.load_state_dict(state_dict, strict=False)  # load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'optimizer', 'training_results', 'epoch'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load('yolov7.pt', map_location='cuda')  # load\n",
    "model = ckpt['model'].float().fuse().eval()  # FP32 model\n",
    "dummy = torch.rand([1, 3, 640, 640]).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(dummy, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15120, 85])\n",
      "torch.Size([1, 3, 48, 80, 85])\n",
      "torch.Size([1, 3, 24, 40, 85])\n",
      "torch.Size([1, 3, 12, 20, 85])\n"
     ]
    }
   ],
   "source": [
    "print(pred[0].shape)\n",
    "for i in pred[1]:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.general import non_max_suppression\n",
    "pred = non_max_suppression(pred, 0.25, 0.45, classes=opt.classes, agnostic=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x2872c7cf250>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model[50].register_forward_hook(get_features('backbone'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDS = []\n",
    "FEATS = []\n",
    "dummy = torch.rand([1, 3, 384, 640]).to('cuda')\n",
    "# forward pass [with feature extraction]\n",
    "preds = model(dummy)\n",
    "\n",
    "# add feats and preds to lists\n",
    "# PREDS.append(preds.detach().cpu().numpy())\n",
    "FEATS.append(features['backbone'].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 12, 20])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['backbone'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freeze = [f'model.{x}.' for x in (freeze if len(freeze) > 1 else range(freeze[0]))]  # parameter names to freeze (full or partial)\n",
    "for k, v in model.named_parameters():\n",
    "    v.requires_grad = True  # train all layers\n",
    "    if any(x in k for x in freeze):\n",
    "        print('freezing %s' % k)\n",
    "        v.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from models.linear_transformer import linear_base\n",
    "lin_trans = linear_base()\n",
    "# state_dict = torch.load('linear-base-checkpoint.pth')\n",
    "# lin_trans.load_state_dict(state_dict=state_dict['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[197], expected input with shape [*, 197], but got input of size[2, 768, 1601]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\luido\\OneDrive\\Documentos\\GitHub\\yolov7\\exploration.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luido/OneDrive/Documentos/GitHub/yolov7/exploration.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m640\u001b[39m, \u001b[39m640\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/luido/OneDrive/Documentos/GitHub/yolov7/exploration.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m out \u001b[39m=\u001b[39m lin_trans(x)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luido/OneDrive/Documentos/GitHub/yolov7/exploration.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-----\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luido/OneDrive/Documentos/GitHub/yolov7/exploration.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnum params: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39msum\u001b[39m(p\u001b[39m.\u001b[39mnumel() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m lin_trans\u001b[39m.\u001b[39mparameters())\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\luido\\Anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\luido\\OneDrive\\Documentos\\GitHub\\yolov7\\models\\linear_transformer.py:170\u001b[0m, in \u001b[0;36mLinearVisionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 170\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_features(x)\n\u001b[0;32m    171\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead(x)\n\u001b[0;32m    172\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\luido\\OneDrive\\Documentos\\GitHub\\yolov7\\models\\linear_transformer.py:163\u001b[0m, in \u001b[0;36mLinearVisionTransformer.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39m# Transformer\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[1;32m--> 163\u001b[0m     x \u001b[39m=\u001b[39m blk(x)\n\u001b[0;32m    165\u001b[0m \u001b[39m# Final layernorm\u001b[39;00m\n\u001b[0;32m    166\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n",
      "File \u001b[1;32mc:\\Users\\luido\\Anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\luido\\OneDrive\\Documentos\\GitHub\\yolov7\\models\\linear_transformer.py:38\u001b[0m, in \u001b[0;36mLinearBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x)))\n\u001b[0;32m     37\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm2(x)))\n\u001b[0;32m     39\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\luido\\Anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\luido\\Anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[0;32m    190\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[1;32mc:\\Users\\luido\\Anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\functional.py:2503\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2499\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[0;32m   2500\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2501\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[0;32m   2502\u001b[0m     )\n\u001b[1;32m-> 2503\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given normalized_shape=[197], expected input with shape [*, 197], but got input of size[2, 768, 1601]"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 640, 640)\n",
    "out = lin_trans(x)\n",
    "print('-----')\n",
    "print(f'num params: {sum(p.numel() for p in lin_trans.parameters())}')\n",
    "print(out.shape)\n",
    "loss = out.sum()\n",
    "loss.backward()\n",
    "print('Single iteration completed successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "path = r'D:\\datasets\\MOT17\\MOT17\\test\\MOT17-01-DPM\\img1'\n",
    "window = 10\n",
    "file_list = os.listdir(path)\n",
    "# file_list.reverse()\n",
    "alpha_range = np.flip(np.arange(105, 256, step=(255-100)//window)[1:])\n",
    "image = Image.open(os.path.join(path, file_list[0]))\n",
    "image.putalpha(alpha_range[0])\n",
    "for img, alpha in zip(reversed(os.listdir(path)[1:]), alpha_range[1:]):\n",
    "    image_1 = Image.open(os.path.join(path, img))\n",
    "    image_1.putalpha(alpha)\n",
    "    image.paste(image_1, (0, 0), image_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = [cv2.cvtColor(cv2.imread(os.path.join(path, file)), cv2.COLOR_BGR2RGB) for file in file_list[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range = np.arange(0.5, 1.01, step=0.5/window)[1:]\n",
    "img_pile = [(i*alpha).astype(int) for i, alpha in  zip(img, alpha_range)]\n",
    "# img1 = (img[0]*0.5).astype(int)\n",
    "# img2 = img[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1920, 3)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(img_pile)//len(img_pile)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('ten-frame-avg.jpg', sum(img_pile)//len(img_pile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_avg = (img2+img1)//2\n",
    "cv2.imwrite('test.jpg', img_avg)\n",
    "cv2.imwrite('frame0.jpg', img[0])\n",
    "cv2.imwrite('frame_last.jpg', img[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def make_gif(frames):\n",
    "    frames = [Image.fromarray(image) for image in frames]\n",
    "    frame_one = frames[0]\n",
    "    frame_one.save(\"dif.gif\", format=\"GIF\", append_images=frames,\n",
    "               save_all=True, duration=100, loop=0)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    make_gif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 20, 20])\n",
      "torch.Size([1, 512, 80, 80])\n",
      "torch.Size([1, 64, 320, 320])\n",
      "torch.Size([1, 16, 640, 640])\n",
      "torch.Size([1, 1, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "# With square kernels and equal stride\n",
    "m = nn.ConvTranspose2d(1024, 256, 3, stride=3)\n",
    "\n",
    "# non-square kernels and unequal stride and with padding\n",
    "# m = nn.ConvTranspose2d(1024, 256, (3, 5), stride=(2, 1), padding=(4, 2))\n",
    "input1 = torch.randn(1, 1024, 20, 20)\n",
    "input2 = torch.randn(1, 1024, 20, 20)\n",
    "\n",
    "x = torch.cat([input1, input2], dim=1)\n",
    "print(input.size())\n",
    "m = nn.ConvTranspose2d(2048, 512, 4, stride=4)\n",
    "x = m(x)\n",
    "print(x.size())\n",
    "m = nn.ConvTranspose2d(512, 64, 4, stride=4)\n",
    "x = m(x)\n",
    "print(x.size())\n",
    "m = nn.ConvTranspose2d(64, 16, 2, stride=2)\n",
    "x = m(x)\n",
    "print(x.size())\n",
    "m = nn.Conv2d(16, 1, 1, stride=1)\n",
    "x = m(x)\n",
    "print(x.size())\n",
    "# m = nn.ConvTranspose2d(512, 256, (3, 2), stride=(3, 2))\n",
    "# output = m(output)\n",
    "# m = nn.ConvTranspose2d(256, 128, (2, 2), stride=(3, 2))\n",
    "# output = m(output)\n",
    "# m = nn.ConvTranspose2d(128, 64, (2, 2), stride=(2,3))\n",
    "# output = m(output)\n",
    "# m = nn.ConvTranspose2d(64, 32, (2, 2), stride=(2, 2))\n",
    "# output = m(output)\n",
    "# m = nn.ConvTranspose2d(32, 16, (2, 1), stride=(2, 2))\n",
    "# output = m(output)\n",
    "# m = nn.Conv2d(2048, 512, 3)\n",
    "# output = m(input)\n",
    "# m = nn.Conv2d(512, 64, 3)\n",
    "# output = m(output)\n",
    "# # output.size()\n",
    "# m = nn.MaxPool2d(2)\n",
    "# # f = nn.Flatten()\n",
    "# output = m(output)\n",
    "# exact output size can be also specified as an argument\n",
    "# input = torch.randn(1, 16, 12, 12)\n",
    "# downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n",
    "# upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n",
    "# h = downsample(input)\n",
    "# h.size()\n",
    "# output = upsample(h, output_size=input.size())\n",
    "# output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.09806, 0.18374, 0.17956,  ..., 0.13409, 0.10269, 0.08879],\n",
       "          [0.20947, 0.15328, 0.16049,  ..., 0.09094, 0.11564, 0.19846],\n",
       "          [0.13016, 0.13465, 0.15796,  ..., 0.10224, 0.16818, 0.15593],\n",
       "          ...,\n",
       "          [0.13561, 0.10059, 0.13223,  ..., 0.14580, 0.17216, 0.10702],\n",
       "          [0.11683, 0.17047, 0.10594,  ..., 0.09445, 0.06275, 0.11017],\n",
       "          [0.15938, 0.06229, 0.11667,  ..., 0.06029, 0.06223, 0.11421]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luido\\Anaconda3\\envs\\torch-gpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "class TrackingHead(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1_3 = nn.Conv2d(in_channels=2048, out_channels=512, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.conv2_3 = nn.Conv2d(in_channels=512, out_channels=64, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.linear1 = nn.Linear(64*4*8, 640*640*1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.relu(self.conv1_3(x)))\n",
    "        x = self.bn2(self.relu(self.conv2_3(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.sigmoid(self.linear1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeler = TrackingHead().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input1 = torch.randn(1, 1024, 12, 20)\n",
    "input2 = torch.randn(1, 1024, 12, 20)\n",
    "\n",
    "input = torch.cat([input1, input2], dim=1).to('cuda')\n",
    "out = modeler(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4946, 0.5104, 0.5643,  ..., 0.7803, 0.5898, 0.8119],\n",
       "         [0.7215, 0.7817, 0.4841,  ..., 0.4375, 0.1060, 0.8303],\n",
       "         [0.6543, 0.4936, 0.5628,  ..., 0.6289, 0.2710, 0.5993],\n",
       "         ...,\n",
       "         [0.8595, 0.5496, 0.6923,  ..., 0.1678, 0.5367, 0.2149],\n",
       "         [0.5750, 0.6820, 0.2098,  ..., 0.2899, 0.2678, 0.8842],\n",
       "         [0.5821, 0.1282, 0.2823,  ..., 0.6395, 0.2768, 0.4981]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.view(1, 640, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k:\\Users\\krish\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models.tracker import YoloTracker\n",
    "\n",
    "model = YoloTracker(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.rand([1, 3, 640, 640]).to('cuda')\n",
    "dummy2 = torch.rand([1, 3, 640, 640]).to('cuda')\n",
    "y_p1, t_p1 = model(dummy)\n",
    "y_p2, t_p2 = model(dummy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 640, 640])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 80, 80, 85])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p[0]\n",
    "\n",
    "pxy = ps[:, :2].sigmoid() * 2. - 0.5\n",
    "pwh = (ps[:, 2:4].sigmoid() * 2) ** 2 * anchors[i]\n",
    "pbox = torch.cat((pxy, pwh), 1)  # predicted box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneral\u001b[39;00m \u001b[39mimport\u001b[39;00m non_max_suppression\n\u001b[1;32m----> 2\u001b[0m pred \u001b[39m=\u001b[39m non_max_suppression(y_p, \u001b[39m0.25\u001b[39;49m, \u001b[39m0.45\u001b[39;49m, classes\u001b[39m=\u001b[39;49m[\u001b[39m0\u001b[39;49m], agnostic\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mk:\\Users\\krish\\Documents\\GitHub\\yolov7\\utils\\general.py:616\u001b[0m, in \u001b[0;36mnon_max_suppression\u001b[1;34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnon_max_suppression\u001b[39m(prediction, conf_thres\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m, iou_thres\u001b[39m=\u001b[39m\u001b[39m0.45\u001b[39m, classes\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, agnostic\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, multi_label\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    609\u001b[0m                         labels\u001b[39m=\u001b[39m()):\n\u001b[0;32m    610\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Runs Non-Maximum Suppression (NMS) on inference results\u001b[39;00m\n\u001b[0;32m    611\u001b[0m \n\u001b[0;32m    612\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[0;32m    613\u001b[0m \u001b[39m         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 616\u001b[0m     nc \u001b[39m=\u001b[39m prediction\u001b[39m.\u001b[39;49mshape[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m5\u001b[39m  \u001b[39m# number of classes\u001b[39;00m\n\u001b[0;32m    617\u001b[0m     xc \u001b[39m=\u001b[39m prediction[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m4\u001b[39m] \u001b[39m>\u001b[39m conf_thres  \u001b[39m# candidates\u001b[39;00m\n\u001b[0;32m    619\u001b[0m     \u001b[39m# Settings\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from utils.general import non_max_suppression\n",
    "pred = non_max_suppression(y_p, 0.25, 0.45, classes=[0], agnostic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('torch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "385b8a38bbc2cc996b2ffe71ea4a23ca5dfa9663044d897c2a7b8d0365f43b8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
